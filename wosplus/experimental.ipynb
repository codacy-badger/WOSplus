{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_sample.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/restrepo/WOSplus/blob/experimental/wosplus/experimental.ipynb)"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "8h8QFxvObxdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2abbe364-1f10-417c-8c92-936597408e94"
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# Check if colaboratory was launched and install missing requirements\n",
        "if [ \"$(pwd)\" == /content ];then\n",
        "    pip install openpyxl xlrd unidecode python-levenshtein requests_testadapter > /dev/null\n",
        "    git clone https://github.com/restrepo/WOSplus.git > /dev/null\n",
        "    #mv WOSplus/wosplus/* .\n",
        "fi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'WOSplus'...\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "TF3UTQqjaLxa",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "if os.getcwd()== '/content':\n",
        "    os.chdir('/content/WOSplus/wosplus')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "s2kdcEq8R2Aq",
        "outputId": "b55ff2a8-b51c-4385-c1f9-8eb7123e49ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "%%writefile drive.cfg\n",
        "[FILES]\n",
        "Sample_WOS.xlsx = 1--LJZ4mYyQcaJ93xBdbnYj-ZzdjO2Wq2\n",
        "Sample_SCI.xlsx = 1-3a-hguQTk5ko8JRLCx--EKaslxGVscf\n",
        "Sample_SCP.xlsx = 1-IAWlMdp2U-9L2jvZUio04ub1Ym3PX-H"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting drive.cfg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "uwbkDKBbbwvQ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "import re\n",
        "import pandas as pd\n",
        "from configparser import ConfigParser\n",
        "try:\n",
        "    from ._google_drive_tools import *\n",
        "    from ._pajek_tools import *\n",
        "    from ._wos_scp import *\n",
        "    from ._merge_tools import *\n",
        "    from ._wos_parser import *\n",
        "except (SystemError, ImportError):\n",
        "    from _google_drive_tools import *\n",
        "    from _pajek_tools import *\n",
        "    from _wos_scp import *\n",
        "    from _merge_tools import *\n",
        "    from _wos_parser import *\n",
        "\n",
        "#TODO: change Tipo for Type or something similar\n",
        "#pd.set_option('display.max_rows', 500)\n",
        "#pd.set_option('display.max_columns', 500)\n",
        "#pd.set_option('display.max_colwidth',1000)\n",
        "def grep(pattern,multilinestring):\n",
        "    '''Grep replacement in python\n",
        "    as in: $ echo $multilinestring | grep pattern\n",
        "    dev: re.M is for multiline strings\n",
        "    '''\n",
        "    grp=re.finditer('(.*)%s(.*)' %pattern, multilinestring,re.M)\n",
        "    return '\\n'.join([g.group(0) for g in grp])\n",
        "\n",
        "def merge_inner_interior_exterior(LEFT,RIGHT,on_condition='SCP_DOI',left_on='ST',right_on='Simple_Title',\\\n",
        "            left_series=pd.Series(),right_series=pd.Series(),\\\n",
        "            left_extra_on='SO',right_extra_on='UDEA_nombre revista o premio',\\\n",
        "            close_matches=False,cutoff=0.6,cutoff_extra=0.6):\n",
        "    '''\n",
        "    Given a df LEFT and a RIGHT[RIGHT[on_condition]!=''] fully True, then\n",
        "    Get a tuple with the following 3 dataframes\n",
        "    1) left-right intersection (inner): LR\n",
        "    1) pure left (interior): L-LR\n",
        "    3) pure right: R-LR (exterior)\n",
        "    '''\n",
        "    if LEFT.shape[0]==0:\n",
        "        print('All entries matched',r,l)\n",
        "        return pd.DataFrame(),pd.DataFrame(),RIGHT\n",
        "    \n",
        "    RIGHT=RIGHT[RIGHT[on_condition]!='']\n",
        "    if RIGHT.shape[0]:\n",
        "        interior,inner,exterior=merge_by_series(LEFT.copy(),RIGHT.copy(),left_on=left_on,right_on=right_on,\\\n",
        "                                   left_series=left_series,right_series=right_series,\\\n",
        "                                   left_extra_on=left_extra_on,right_extra_on=right_extra_on,\\\n",
        "                                   close_matches=close_matches,cutoff=cutoff,cutoff_extra=cutoff_extra)\n",
        "        if LEFT.shape[0]>=interior.shape[0] and RIGHT.shape[0]>=exterior.shape[0]:\n",
        "            return inner.reset_index(drop=True),interior.reset_index(drop=True),exterior.reset_index(drop=True)\n",
        "    else:\n",
        "        return (pd.DataFrame(),pd.DataFrame(),pd.DataFrame())\n",
        "\n",
        "\n",
        "#Start here\n",
        "class wosplus:\n",
        "    \"\"\"\n",
        "    Input files assumed to have public links in Google Drive\n",
        "    A config file, e.g 'drive.cfg' is expected with the following structure\n",
        "    ==============================================================\n",
        "    [FILES]\n",
        "    WOS_FILE.xlsx              = 1--LJZ4mYyQcaJ93xBdbnYj-ZzdjO2Wq2\n",
        "    ...\n",
        "    ==============================================================\n",
        "    USAGE:\n",
        "        import wosplus as wp\n",
        "        WOS=wp.wosplus('drive.cfg')\n",
        "        #check Google Drive id for file\n",
        "        WOS.drive_file.get('WOS_FILE.xlsx')\n",
        "        #load biblio\n",
        "        WOS.load_biblio('WOS_FILE.xlsx')\n",
        "        # DataFrame in WOS.WOS or WOS.biblio['WOS']\n",
        "  \n",
        "    The main method is 'load_biblio' it must have a prefix according to\n",
        "    the type of supported bibliography. This prefix will be appended\n",
        "    to the ALL the columns of the generated bibligraphy\n",
        "\n",
        "    wp.type gives the type of bibliography Data Base.\n",
        "    Currently implenented:\n",
        "      * WOS: two characters columns\n",
        "      * SCI: prefixed SCI_ columns is returned\n",
        "      * SCP: prefixed SCP_ columns is returned \n",
        "    and any combinantion\n",
        "    of them keeping the same ordering.\n",
        "\n",
        "    The type mus be declared with the 'load_biblio' with the 'prefix' option\n",
        "    (Default type is WOS)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,cfg_file=''):\n",
        "        self.df=pd.DataFrame()\n",
        "        '''\n",
        "        Based on:\n",
        "        http://stackoverflow.com/a/39225272\n",
        "        '''\n",
        "        cfg=ConfigParser()\n",
        "        cfg.optionxform=str\n",
        "        if cfg_file:\n",
        "            tmp=cfg.read(cfg_file)\n",
        "        else:\n",
        "            tmp=cfg.read_dict({'FILES':\n",
        "                    {'Sample_WOS.xlsx':'0BxoOXsn2EUNIMldPUFlwNkdLOTQ'}})\n",
        "            \n",
        "        self.drive_file=cfg['FILES']\n",
        "        self.type=pd.Series()\n",
        "        self.biblio=pd.Series()\n",
        "\n",
        "    def read_drive_excel(self,file_name,**kwargs):\n",
        "        '''\n",
        "        TODO: Make independent of the class!\n",
        "        Generalization of the Pandas DataFrame read_excel method\n",
        "        to include google drive file names:\n",
        "         \n",
        "         Read excel or csv file from google drive\n",
        "         Requires a self.drive_file dictionary intialized with the class\n",
        "         (see below) with the id's for the\n",
        "         google drive file names.\n",
        "         If the file_name is not found in the drive_file dictionary it is read locally.\n",
        "         If the file_name have an extension .csv, try to read the google spreadsheet\n",
        "         directly: check pandas_from_google_drive_csv for passed options\n",
        "         WARNING: ONLY OLD Google Spread Sheet allows to load sheet different from 0\n",
        "\n",
        "         drive_file dictionary: for some file, e.g 'drive.cfg' the format must be:\n",
        "          $ cat drive.cfg\n",
        "          [FILES]\n",
        "          Sample_WOS.xlsx = 0BxoOXsn2EUNIMldPUFlwNkdLOTQ\n",
        "        '''\n",
        "        # Try to load Google spreadsheet if extension is csv\n",
        "        if re.search('\\.csv$',file_name):\n",
        "            if self.drive_file.get(file_name):\n",
        "                return pandas_from_google_drive_csv(\n",
        "                    self.drive_file.get(file_name),**kwargs)\n",
        "            else:\n",
        "                return pd.read_csv(file_name,**kwargs)\n",
        "       \n",
        "        # Try to load xlsx file if file extension is not csv\n",
        "        if self.drive_file.get(file_name):\n",
        "            return pd.read_excel( download_file_from_google_drive(\n",
        "                self.drive_file.get(file_name) ) ,**kwargs)  # ,{} is an accepted option\n",
        "        else:\n",
        "            return pd.read_excel(file_name,**kwargs)\n",
        "    def load_biblio(self,WOS_file,prefix='WOS'):\n",
        "        \"\"\"\n",
        "        Load WOS xlsx file, or if prefix is given:\n",
        "          prefix='SCI': Load SCI xlsx file and append the 'SCI_' prefix in each column\n",
        "          prefix='SCP': Load SCI csv file and append the 'SCP_' prefix in each column\n",
        "        and add the WOS, SCI, or SCP  attribute to self.\n",
        "        \"\"\"\n",
        "        from pathlib import Path\n",
        "        import sys\n",
        "        DOI='DI'\n",
        "        if prefix=='SCP': #Only if pure scopus\n",
        "            DOI='DOI'\n",
        "        #elif: #Other no WOS-like pures\n",
        "\n",
        "        if not re.search('\\.txt$',WOS_file):\n",
        "            WOS=self.read_drive_excel(WOS_file)\n",
        "        else:\n",
        "            id_google_drive=self.drive_file.get('{}'.format(WOS_file))\n",
        "            if id_google_drive:                                     \n",
        "                wos_txt=download_file_from_google_drive(  id_google_drive )#,binary=False)\n",
        "                WOS=wos_to_list_to_pandas(wos_txt)\n",
        "            else: #check local file\n",
        "                my_file = Path(WOS_file)\n",
        "                if my_file.is_file():\n",
        "                    WOS=wos_parser(WOS_file)\n",
        "                else:\n",
        "                    sys.exit('WOS File: {}, NOT FOUND!'.format(WOS_file))\n",
        "\n",
        "        WOS=fill_NaN(WOS)\n",
        "        if prefix=='SCI':\n",
        "            exec('self.{}_not_prefix=WOS'.format(prefix))\n",
        "        \n",
        "        \n",
        "        if 'DI' in WOS and 'TI' in WOS and 'SO' in WOS:\n",
        "            WOS['DI']=WOS['DI'].str.strip()\n",
        "            WOS['TI']=WOS['TI'].str.strip().str.replace('\\n',' ')\n",
        "            WOS['SO']=WOS['SO'].str.strip()\n",
        "        if 'X1' in WOS:\n",
        "            WOS_ti,WOS_not_ti=df_split(WOS,on='TI',on_not_condition=True)\n",
        "            WOS_not_ti['TI']=WOS_not_ti['X1']\n",
        "            WOS=WOS_ti.append(WOS_not_ti).reset_index(drop=True)\n",
        "        \n",
        "        if 'TI' in WOS:\n",
        "            WOS=WOS.drop_duplicates('TI')\n",
        "\n",
        "        #Drop duplicated DOIS\n",
        "        if DOI in WOS:\n",
        "            WOS_di,WOS_not_di=df_split(WOS,on=DOI,on_not_condition=True)\n",
        "            WOS=WOS_not_di.append(WOS_di.drop_duplicates(DOI)).reset_index(drop=True)\n",
        "\n",
        "            \n",
        "        if prefix != 'WOS' and not re.search('_',prefix):\n",
        "            #check if already present\n",
        "            WOS=columns_add_prefix(WOS,prefix)\n",
        "\n",
        "        # Without prefix columns\n",
        "        if not WOS.get('Tipo') and not re.search('_',prefix):\n",
        "            WOS['Tipo']=prefix\n",
        "        else:\n",
        "            print('WARNING: Biblio already has a \"Tipo\" column')\n",
        "            \n",
        "        exec('self.{}=WOS'.format(prefix))\n",
        "        self.type['{}'.format(prefix)]='{}'.format(prefix)\n",
        "        self.biblio['{}'.format(prefix)]=WOS\n",
        "        \n",
        "    def merge(self,left='WOS',right='SCI',\n",
        "                   right_DOI=None,right_TI=None,right_extra_journal=None,\n",
        "                   right_author=None,right_year=None,\n",
        "              DEBUG=False):\n",
        "        \"\"\"\n",
        "        Merge left and right bibliographic dataframes by TYPE and with \n",
        "        Python merge ooption: `how='outer'`.\n",
        "        \n",
        "        The TYPE must coincide with the Object attribute Dataframe: eg:\n",
        "        `left='WOS'` imply that WOS must be an attribute of\n",
        "        self: self.WOS\n",
        "        `right='SCI'` imply that WOS must be an attribute of\n",
        "        self: self.WOS\n",
        "        The DataFrame attributtes of the object `self` are populated by using\n",
        "          `self.loadbiblio(file)`: See `self` help for further instructions.\n",
        "\n",
        "        The self.right DataFrame need to have some mandatories columns:\n",
        "         [[right_DOI,right_TI,right_extra_journal,right_author,right_year]]\n",
        "         They are automatically defined for self.righ TYPE: SCI and SCP and\n",
        "         must be given for other TYPE\n",
        "        \n",
        "        Output:\n",
        "        The resulting DataFrame is returned as:\n",
        "          * self.left_right # with strings names converted into variable names\n",
        "          * self.bibilio['left_right'] # pd.Series\n",
        "        and also the new resulting TYPE is stored as\n",
        "          * self.Tipo['left_right'] -> 'left_right' # pd.Series\n",
        "          \n",
        "        The merged DOI, Titles and Journal Names are stored in\n",
        "        the WOS like `self.left_right` columns: DI,TI,SO with `self.left`\n",
        "        priority for the values.\n",
        "        \"\"\"\n",
        "        if not hasattr(self,left) or not hasattr(self,right):\n",
        "            sys.exit('ERROR:  {} and {} must be attributes of class {}'.format(left,right,self.__class__.__name__))\n",
        "            \n",
        "        if left not in self.biblio or right not in self.biblio:\n",
        "            sys.exit('ERROR: missing biblio Series in {}'.format(left,right,self.__class__.__name__)  )\n",
        "\n",
        "        if left not in self.type or right not in self.type:\n",
        "            sys.exit('ERROR: missing type Series in {}'.format(left,right,self.__class__.__name__)  )\n",
        "            \n",
        "        left_df=self.biblio[left].copy()\n",
        "        right_df=self.biblio[right].copy()\n",
        "        if DEBUG:\n",
        "            print('intial: {}'.format(left_df.shape[0]+right_df.shape[0]))\n",
        "        if left=='WOS' or  re.search('^WOS_',left):\n",
        "            left_DOI='DI'\n",
        "            left_TI='TI'\n",
        "            left_extra_journal='SO' #helps with similiraty search  by Title\n",
        "        #elif\n",
        "        #else:\n",
        "            #sys.error('not supported left type')\n",
        "        #clean Tipo\n",
        "        if 'Tipo' in right_df:\n",
        "            right_df=right_df.drop('Tipo',axis='columns')\n",
        "            \n",
        "        if right=='SCI':\n",
        "            right_DOI='SCI_DI'\n",
        "            right_TI='SCI_TI'\n",
        "            right_extra_journal='SCI_SO' #helps with similiraty search  by Title\n",
        "            right_author='SCI_AU'\n",
        "            right_year='SCI_PY'\n",
        "        elif right=='SCP':\n",
        "            if 'SCP_Title' in right_df and not 'SCP_Title_0' in right_df:\n",
        "                right_df=split_translated_columns(right_df.copy(),on='SCP_Title',sep='\\[',min_title=16)\n",
        "            right_DOI='SCP_DOI'\n",
        "            right_TI='SCP_Title'\n",
        "            right_extra_journal='SCP_Source title' #helps with similiraty search  by Title\n",
        "            right_author='SCP_Authors'\n",
        "            right_year='SCP_Year'\n",
        "\n",
        "        #else:\n",
        "            #sys.error('not supported right type')\n",
        "\n",
        "        # Merge on DOIs                                 \n",
        "            \n",
        "        LEFT_RIGHT_inner=pd.DataFrame() \n",
        "        RIGHT_on=right_DOI\n",
        "        # RIGHT: no empty values for column 'right_df'\n",
        "        RIGHT,next_RIGHT=df_split(right_df,on=RIGHT_on,on_not_condition=True)\n",
        "        #full_RIGHT=RIGHT.append(next_RIGHT)\n",
        "        LEFT=left_df\n",
        "        LEFT_on=left_DOI\n",
        "        LEFT_series=clean(LEFT[LEFT_on])\n",
        "        RIGHT_series=clean(RIGHT[RIGHT_on])\n",
        "\n",
        "        LR=merge_inner_interior_exterior(LEFT.copy(),RIGHT.copy(),\\\n",
        "                    on_condition=RIGHT_on,left_on='LEFT_simple_doi',right_on='RIGHT_simple_doi',\\\n",
        "                                   left_series=LEFT_series,right_series=RIGHT_series)\n",
        "        if LR[0].shape[0]:\n",
        "            inner,new_LEFT,new_RIGHT=LR # LEFT.shape[0]=inner.shape[0]+new_LEFT.shape[0]\n",
        "                                        # RIGHT.shape[0]=inner.shape[0]+new_RIGHT.shape[0]\n",
        "            inner['Tipo']=inner['Tipo']+'_{}'.format(right)        \n",
        "            LEFT_RIGHT_inner=LEFT_RIGHT_inner.append(inner).reset_index(drop=True)\n",
        "        else:\n",
        "            new_LEFT=LEFT; new_RIGHT= RIGHT\n",
        "            \n",
        "\n",
        "        if DEBUG:\n",
        "            print(inner.shape[0],new_LEFT.shape[0],new_RIGHT.shape[0],'=,...,=')\n",
        "            print(LEFT.shape,RIGHT.shape)\n",
        "            print(new_LEFT.shape,(next_RIGHT.append(new_RIGHT)).shape,LEFT_RIGHT_inner.shape)\n",
        "            print( ( ( new_LEFT.append(LEFT_RIGHT_inner)  ).append(new_RIGHT) ).append(next_RIGHT).shape )\n",
        "            \n",
        "        #Merge on (splitted) Titles: generated with 'split_translated_columns' before\n",
        "        #next_RIGHT have column information even if empty\n",
        "        for nTI in [right_TI]+[ x for x in next_RIGHT.columns\n",
        "                                if re.search( '{}_[0-9]+'.format(right_TI),x  )]:\n",
        "            RIGHT_on=nTI\n",
        "            full_RIGHT=new_RIGHT.append(next_RIGHT)\n",
        "            RIGHT,next_RIGHT=df_split(full_RIGHT,on=RIGHT_on,on_not_condition=True)\n",
        "            RIGHT=RIGHT.drop_duplicates(RIGHT_on)\n",
        "            RIGHT_series=clean(RIGHT[RIGHT_on])\n",
        "\n",
        "            LEFT=new_LEFT\n",
        "            LEFT_series=clean(LEFT[left_TI])\n",
        "\n",
        "            LR=merge_inner_interior_exterior(LEFT.copy(),RIGHT.copy(),\\\n",
        "                                on_condition=RIGHT_on,left_on='LEFT_Simple_title',right_on='RIGHT_Simple_title',\\\n",
        "                                  left_series=LEFT_series,right_series=RIGHT_series)\n",
        "            if LR[0].shape[0]:\n",
        "                inner,new_LEFT,new_RIGHT=LR\n",
        "                inner['Tipo']=inner['Tipo']+'_{}'.format(right)\n",
        "                LEFT_RIGHT_inner=LEFT_RIGHT_inner.append(inner).reset_index(drop=True)\n",
        "            else:\n",
        "                new_LEFT=LEFT; new_RIGHT= RIGHT\n",
        "\n",
        "                \n",
        "            if DEBUG:\n",
        "                print(inner.shape[0],new_LEFT.shape[0],new_RIGHT.shape[0])\n",
        "                print(LEFT.shape,RIGHT.shape)\n",
        "                print(new_LEFT.shape,(next_RIGHT.append(new_RIGHT)).shape,LEFT_RIGHT_inner.shape) \n",
        "                print( ( ( new_LEFT.append(LEFT_RIGHT_inner)  ).append(new_RIGHT) ).append(next_RIGHT).shape )\n",
        "        \n",
        "        # Merge on Similar Titles\n",
        "        for nTI in [right_TI]+[ x for x in next_RIGHT.columns \n",
        "                                if re.search( '{}_[0-9]+'.format(right_TI),x  )]:                \n",
        "            RIGHT_on=nTI\n",
        "            full_RIGHT=new_RIGHT.append(next_RIGHT)\n",
        "            RIGHT,next_RIGHT=df_split(full_RIGHT,on=RIGHT_on,on_not_condition=True)\n",
        "            RIGHT=RIGHT.drop_duplicates(RIGHT_on)\n",
        "            RIGHT_series=clean(RIGHT[RIGHT_on]).str.replace('\\[','').str.replace('\\]','')\n",
        "\n",
        "            LEFT=new_LEFT\n",
        "            LEFT_series=clean(LEFT[left_TI])\n",
        "\n",
        "            LR=merge_inner_interior_exterior(LEFT.copy(),RIGHT.copy(),\\\n",
        "                                on_condition=RIGHT_on,left_on='LEFT_Simple_title',right_on='RIGHT_Simple_title',\\\n",
        "                                  left_series=LEFT_series,right_series=RIGHT_series,\\\n",
        "                                  left_extra_on=left_extra_journal,right_extra_on=right_extra_journal,\\\n",
        "                                  close_matches=True,cutoff=0.6)\n",
        "            if LR[0].shape[0]:\n",
        "                inner,new_LEFT,new_RIGHT=LR\n",
        "                inner['Tipo']=inner['Tipo']+'_{}'.format(right)\n",
        "                LEFT_RIGHT_inner=LEFT_RIGHT_inner.append(inner).reset_index(drop=True)\n",
        "            else:\n",
        "                new_LEFT=LEFT; new_RIGHT= RIGHT\n",
        "\n",
        "\n",
        "            if DEBUG:\n",
        "                print(inner.shape[0],new_LEFT.shape[0],new_RIGHT.shape[0])\n",
        "                print(LEFT.shape,RIGHT.shape)\n",
        "                print(new_LEFT.shape,(next_RIGHT.append(new_RIGHT)).shape,LEFT_RIGHT_inner.shape) \n",
        "                print( ( ( new_LEFT.append(LEFT_RIGHT_inner)  ).append(new_RIGHT) ).append(next_RIGHT).shape )\n",
        "                \n",
        "        \n",
        "        \n",
        "        full_RIGHT=next_RIGHT.append(new_RIGHT)\n",
        "        full_RIGHT['Tipo']=right\n",
        "        full_RIGHT['DI']=full_RIGHT[right_DOI]\n",
        "        full_RIGHT['TI']=full_RIGHT[right_TI]\n",
        "        full_RIGHT['SO']=full_RIGHT[right_extra_journal]\n",
        "        full_RIGHT['AU']=full_RIGHT[right_author]\n",
        "        full_RIGHT['PY']=full_RIGHT[right_year]\n",
        "\n",
        "        LEFT_RIGHT=new_LEFT\n",
        "        LEFT_RIGHT=LEFT_RIGHT.append(LEFT_RIGHT_inner)\n",
        "        LEFT_RIGHT=LEFT_RIGHT.append(full_RIGHT)\n",
        "        LEFT_RIGHT=fill_NaN(LEFT_RIGHT).reset_index(drop=True)\n",
        "         \n",
        "        if DEBUG:    \n",
        "            self.LEFT=LEFT\n",
        "            self.RIGHT=RIGHT\n",
        "            self.new_LEFT=new_LEFT\n",
        "            self.new_RIGHT=new_RIGHT\n",
        "            self.LEFT_RIGHT_inner=LEFT_RIGHT_inner\n",
        "            self.full_RIGHT=full_RIGHT\n",
        "        \n",
        "        exec('self.{}_{}=LEFT_RIGHT'.format(left,right))\n",
        "        self.type['{}_{}'.format(left,right)]='{}_{}'.format(left,right)\n",
        "        self.biblio['{}_{}'.format(left,right)]=LEFT_RIGHT\n",
        "        \n",
        "\n",
        "if __name__=='__main__':\n",
        "    WOS_file='Sample_WOS.xlsx'\n",
        "    SCI_file='Sample_SCI.xlsx'\n",
        "    SCP_file='Sample_SCP.csv'\n",
        "    cib=wosplus('drive.cfg')\n",
        "    cib.load_biblio(WOS_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "70Kt9brKaxqN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class normalize(wosplus):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        \"See: http://stackoverflow.com/questions/23027846/def-init-self-args-kwargs-initialization-of-class-in-python\"\n",
        "        super(normalize, self).__init__(*args, **kwargs)        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sJM59dYGbdC1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7143f569-97e1-4fd4-c024-c32ca0c45703"
      },
      "cell_type": "code",
      "source": [
        "normalize('drive.cfg')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.normalize at 0x7f155c9a1c88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "BUzIDf92bwvU",
        "outputId": "58d95832-5a8e-44b5-850a-274e57723961",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "cib=wosplus.wosplus('drive.cfg')\n",
        "\n",
        "cib.load_biblio('Sample_WOS.xlsx')\n",
        "cib.load_biblio('Sample_SCI.xlsx',prefix='SCI')\n",
        "cib.load_biblio('Sample_SCP.xlsx',prefix='SCP')\n",
        "\n",
        "print('before merge: {}'.format( cib.WOS.shape[0]+cib.SCI.shape[0]+cib.SCP.shape[0] )  )\n",
        "\n",
        "cib.merge(left='WOS',right='SCI')\n",
        "\n",
        "if True:\n",
        "    print('intial: {}'.format( cib.WOS.shape[0]+cib.SCI.shape[0]) )\n",
        "    print('final : {}'.format(  cib.WOS_SCI.shape) )\n",
        "\n",
        "cib.merge(left='WOS_SCI',right='SCP')\n",
        "\n",
        "if True:\n",
        "    print('intial: {}'.format( cib.WOS_SCI.shape[0]+cib.SCP.shape[0]) )\n",
        "    print('final : {}'.format(  cib.WOS_SCI_SCP.shape) )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/restrepo/tmp/testing/WOSplus/wosplus/_wos_scp.py:54: FutureWarning: Using 'rename_axis' to alter labels is deprecated. Use '.rename' instead\n",
            "  return df.rename_axis( dict( (key,prefix+'_'+key) for key in df.columns.values) , axis=1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "before merge: 48\n",
            ".intial: 38\n",
            "final : (28, 96)\n",
            "..intial: 38\n",
            "final : (30, 142)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "tXQxDy_ts_th"
      },
      "cell_type": "markdown",
      "source": [
        "before merge: 48\n",
        ".intial: 38\n",
        "final : (28, 96)\n",
        "..intial: 38\n",
        "final : (30, 142)"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "lyYwEqdojkH2"
      },
      "cell_type": "markdown",
      "source": [
        "## Unitary tests\n",
        "Copy to test.py:\n",
        "```bash\n",
        "cd tests\n",
        "python test.py\n",
        "```"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ugh0hx54ghMH",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import unittest\n",
        "self = unittest.TestCase('__init__')\n",
        "self.assertTrue(True)\n",
        "\n",
        "self.assertTrue( cib.WOS.shape[0]+cib.SCI.shape[0]+cib.SCP.shape[0] == 48 )\n",
        "                  \n",
        "self.assertTrue ( cib.WOS.shape[0]+cib.SCI.shape[0] == 38  )\n",
        "self.assertTrue (  cib.WOS_SCI.shape[0] == 28 ) \n",
        "        \n",
        "         \n",
        "self.assertTrue( cib.WOS_SCI.shape[0]+cib.SCP.shape[0] == 38  )\n",
        "self.assertTrue( cib.WOS_SCI_SCP.shape[0] == 30  )\n",
        "\n",
        "self.assertTrue(list( cib.WOS_SCI_SCP.Tipo.values )==['WOS','WOS',\n",
        "           'WOS','WOS','WOS','WOS','WOS_SCI','SCI','WOS','WOS','WOS','WOS',\n",
        "           'WOS','WOS','WOS','WOS','WOS','WOS_SCI','WOS_SCI','WOS_SCI',\n",
        "           'WOS_SCP','WOS_SCI_SCP','WOS_SCI_SCP','WOS_SCI_SCP','WOS_SCP',\n",
        "           'WOS_SCI_SCP','WOS_SCI_SCP','WOS_SCI_SCP','SCP','SCP'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "7746G3jUa4dr",
        "outputId": "44e3cdc2-43de-4b41-f7cc-d3138521fb45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cd tests\n",
        "python3 test.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "..."
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ".\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 34.223s\n",
            "\n",
            "OK\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "WWNF7iZkPbr9"
      },
      "cell_type": "markdown",
      "source": [
        "### In progress... Other database"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "mwVr3x5cbwvY",
        "outputId": "63879690-c517-462b-9ef6-1f43ab877d18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "cib.load_biblio('Sample_SCP.xlsx',prefix='NEW')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/_wos_scp.py:54: FutureWarning: Using 'rename_axis' to alter labels is deprecated. Use '.rename' instead\n",
            "  return df.rename_axis( dict( (key,prefix+'_'+key) for key in df.columns.values) , axis=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "-PpQ4BfdPh_b",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cib.merge(left='WOS_SCI_SCP',right='NEW',right_DOI='NEW_DOI',\n",
        "            right_TI='NEW_Title',\n",
        "            right_extra_journal='NEW_Source title',\n",
        "            right_author='NEW_Authors',\n",
        "            right_year='NEW_Year')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "6WWU3mIXPoaL",
        "outputId": "fbba0d3c-9c37-4833-e62b-a253ea3ffe2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "cib.WOS_SCI_SCP_NEW.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30, 185)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "4-C0_KEHPpv3",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}